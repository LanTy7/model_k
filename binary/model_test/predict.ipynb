{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BiLSTM 二分类模型 - 批量预测\n",
        "\n",
        "加载训练好的模型，对FASTA文件进行批量ARG预测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from Bio import SeqIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 配置\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================== 路径配置（请修改）==================\n",
        "MODEL_PATH = \"\"  # 训练保存的模型\n",
        "INPUT_DIR = \"\"               # 输入FASTA文件夹\n",
        "OUTPUT_DIR = \"./predicted_results\"                   # 输出结果文件夹\n",
        "THRESHOLD = 0.5                                      # 预测阈值\n",
        "\n",
        "# 设备\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 创建输出目录\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 模型与数据处理定义\n",
        "\n",
        "**注意**: 这里的定义必须与训练代码完全一致！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 氨基酸编码字典（必须与训练一致）\n",
        "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "AA_DICT = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}\n",
        "AA_DICT.update({'X': 21, 'PAD': 0})\n",
        "\n",
        "def seq_to_indices(sequence, max_length):\n",
        "    \"\"\"将氨基酸序列转换为索引数组\"\"\"\n",
        "    indices = [AA_DICT.get(aa, 21) for aa in sequence]\n",
        "    indices = indices[:max_length]\n",
        "    if len(indices) < max_length:\n",
        "        indices += [0] * (max_length - len(indices))\n",
        "    return np.array(indices, dtype=np.int64)\n",
        "\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    \"\"\"BiLSTM + Global Pooling 二分类模型（必须与训练一致）\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            config['vocab_size'], \n",
        "            config['embedding_dim'], \n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=config['embedding_dim'],\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_layers=config['num_layers'],\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=config['dropout'] if config['num_layers'] > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(config['hidden_size'] * 4, config['hidden_size']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(config['hidden_size'], 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        output, _ = self.lstm(emb)\n",
        "        max_pool, _ = torch.max(output, dim=1)\n",
        "        avg_pool = torch.mean(output, dim=1)\n",
        "        features = torch.cat([max_pool, avg_pool], dim=1)\n",
        "        return self.classifier(self.dropout(features))\n",
        "\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "    \"\"\"推理用数据集\"\"\"\n",
        "    def __init__(self, fasta_file, max_length):\n",
        "        self.records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_str = str(self.records[idx].seq).upper()\n",
        "        return torch.from_numpy(seq_to_indices(seq_str, self.max_length))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 加载模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载模型（自动读取保存的配置）\n",
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
        "config = checkpoint['config']\n",
        "print(f\"Model config: {config}\")\n",
        "\n",
        "model = BiLSTMModel(config).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 批量预测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_file(model, input_path, output_path, config, threshold):\n",
        "    \"\"\"对单个FASTA文件进行预测\"\"\"\n",
        "    dataset = InferenceDataset(input_path, config['max_length'])\n",
        "    if len(dataset) == 0:\n",
        "        return 0\n",
        "    \n",
        "    loader = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=4)\n",
        "    arg_records = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_i, inputs in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            logits = model(inputs)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "            \n",
        "            for i, prob in enumerate(probs):\n",
        "                if prob > threshold:\n",
        "                    global_idx = batch_i * loader.batch_size + i\n",
        "                    record = dataset.records[global_idx]\n",
        "                    record.description += f\" [ARG_prob={prob:.3f}]\"\n",
        "                    arg_records.append(record)\n",
        "    \n",
        "    if arg_records:\n",
        "        SeqIO.write(arg_records, output_path, \"fasta\")\n",
        "    \n",
        "    return len(arg_records)\n",
        "\n",
        "\n",
        "# 获取所有待处理文件\n",
        "all_files = glob.glob(os.path.join(INPUT_DIR, \"*.faa\")) + glob.glob(os.path.join(INPUT_DIR, \"*.fasta\"))\n",
        "print(f\"Found {len(all_files)} files to process\")\n",
        "\n",
        "# 批量处理\n",
        "total_args = 0\n",
        "for i, input_path in enumerate(all_files):\n",
        "    filename = os.path.basename(input_path)\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"{filename}_pred.fasta\")\n",
        "    \n",
        "    # 断点续传：跳过已处理文件\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "        continue\n",
        "    \n",
        "    n_args = predict_file(model, input_path, output_path, config, THRESHOLD)\n",
        "    total_args += n_args\n",
        "    \n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Progress: {i+1}/{len(all_files)}\")\n",
        "\n",
        "print(f\"\\nDone! Total ARGs found: {total_args}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
