{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM 二分类模型训练\n",
    "\n",
    "用于抗性基因(ARG)识别的二分类模型\n",
    "\n",
    "**关键改进**:\n",
    "1. 模型配置统一管理，保存时包含配置信息\n",
    "2. 支持1:N比例采样阴性样本\n",
    "3. 集成完整评估流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, auc)\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置参数\n",
    "\n",
    "**重要**: 修改模型配置后，预测代码也需要同步修改！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 模型配置（预测时必须保持一致）==================\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': 22,        # 20氨基酸 + X + PAD\n",
    "    'embedding_dim': 48,     # Embedding维度（降低防过拟合）\n",
    "    'hidden_size': 48,       # LSTM隐藏层维度（降低防过拟合）\n",
    "    'num_layers': 1,         # LSTM层数（单层足够）\n",
    "    'dropout': 0.5,          # Dropout比例\n",
    "    'max_length': 1000,      # 序列最大长度\n",
    "}\n",
    "\n",
    "# ================== 训练配置 ==================\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 256,\n",
    "    'lr': 0.001,             # 模型变小，学习率可稍高\n",
    "    'weight_decay': 5e-3,    # 更强的L2正则化\n",
    "    'epochs': 100,\n",
    "    'patience': 15,          # 早停耐心值\n",
    "    'pos_neg_ratio': 3,      # 阴性样本倍数\n",
    "}\n",
    "\n",
    "# ================== 路径配置（请修改为实际路径）==================\n",
    "PATH_CONFIG = {\n",
    "    'arg_file': \"/home/lanty/Documents/study/model/data/unique_0.95.fasta\",           # 阳性样本\n",
    "    'non_arg_file': \"/home/lanty/Documents/study/model/data/1_3.fasta\",  # 阴性样本\n",
    "    'save_dir': \"./well-trained\",\n",
    "    'fig_dir': \"./figures\",\n",
    "}\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 创建目录\n",
    "for d in [PATH_CONFIG['save_dir'], PATH_CONFIG['fig_dir']]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 日志\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氨基酸编码字典\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_DICT = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}\n",
    "AA_DICT.update({'X': 21, 'PAD': 0})\n",
    "\n",
    "def seq_to_indices(sequence, max_length):\n",
    "    \"\"\"将氨基酸序列转换为索引数组\"\"\"\n",
    "    indices = [AA_DICT.get(aa, 21) for aa in sequence]  # 未知氨基酸映射为X(21)\n",
    "    indices = indices[:max_length]  # 截断\n",
    "    if len(indices) < max_length:   # 填充\n",
    "        indices += [0] * (max_length - len(indices))\n",
    "    return np.array(indices, dtype=np.int64)\n",
    "\n",
    "\n",
    "class FastaDataset(Dataset):\n",
    "    \"\"\"FASTA序列数据集\"\"\"\n",
    "    def __init__(self, fasta_files, label, max_length, sample_n=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fasta_files: FASTA文件路径列表\n",
    "            label: 标签 (0 或 1)\n",
    "            max_length: 序列最大长度\n",
    "            sample_n: 随机采样数量（None表示全部使用）\n",
    "        \"\"\"\n",
    "        self.sequences = []\n",
    "        self.label = label\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 加载序列\n",
    "        for f in fasta_files:\n",
    "            for record in SeqIO.parse(f, \"fasta\"):\n",
    "                self.sequences.append(str(record.seq).upper())\n",
    "        \n",
    "        # 随机采样\n",
    "        if sample_n and sample_n < len(self.sequences):\n",
    "            self.sequences = random.sample(self.sequences, sample_n)\n",
    "        \n",
    "        print(f\"Loaded: label={label}, count={len(self.sequences)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_indices = seq_to_indices(self.sequences[idx], self.max_length)\n",
    "        return torch.from_numpy(seq_indices), torch.tensor(self.label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    \"\"\"BiLSTM + Global Pooling 二分类模型\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            config['vocab_size'], \n",
    "            config['embedding_dim'], \n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config['embedding_dim'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=config['dropout'] if config['num_layers'] > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        # 双向LSTM输出 * 2 (max + avg pooling) = hidden_size * 4\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config['hidden_size'] * 4, config['hidden_size']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['hidden_size'], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)          # (batch, seq_len, embed_dim)\n",
    "        output, _ = self.lstm(emb)       # (batch, seq_len, hidden*2)\n",
    "        \n",
    "        # Global Pooling\n",
    "        max_pool, _ = torch.max(output, dim=1)\n",
    "        avg_pool = torch.mean(output, dim=1)\n",
    "        features = torch.cat([max_pool, avg_pool], dim=1)  # (batch, hidden*4)\n",
    "        \n",
    "        return self.classifier(self.dropout(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练与评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # === 1. 加载数据 ===\n",
    "    logger.info(\"Loading data...\")\n",
    "    \n",
    "    # 加载阳性样本\n",
    "    ds_pos = FastaDataset(\n",
    "        [PATH_CONFIG['arg_file']], \n",
    "        label=1, \n",
    "        max_length=MODEL_CONFIG['max_length']\n",
    "    )\n",
    "    n_pos = len(ds_pos)\n",
    "    \n",
    "    # 加载阴性样本（按比例采样）\n",
    "    n_neg_sample = n_pos * TRAIN_CONFIG['pos_neg_ratio']\n",
    "    ds_neg = FastaDataset(\n",
    "        [PATH_CONFIG['non_arg_file']], \n",
    "        label=0, \n",
    "        max_length=MODEL_CONFIG['max_length'],\n",
    "        sample_n=n_neg_sample\n",
    "    )\n",
    "    \n",
    "    # 合并数据集\n",
    "    full_ds = ConcatDataset([ds_pos, ds_neg])\n",
    "    labels_all = [1] * len(ds_pos) + [0] * len(ds_neg)\n",
    "    \n",
    "    # 分层划分训练集/验证集\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(full_ds)), test_size=0.2, stratify=labels_all, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        Subset(full_ds, train_idx), \n",
    "        batch_size=TRAIN_CONFIG['batch_size'],\n",
    "        shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        Subset(full_ds, val_idx), \n",
    "        batch_size=TRAIN_CONFIG['batch_size'],\n",
    "        shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 计算类别权重\n",
    "    train_labels = [labels_all[i] for i in train_idx]\n",
    "    n_pos_train = sum(train_labels)\n",
    "    n_neg_train = len(train_labels) - n_pos_train\n",
    "    pos_weight = torch.tensor(n_neg_train / n_pos_train, dtype=torch.float32).to(device)\n",
    "    logger.info(f\"Train: Pos={n_pos_train}, Neg={n_neg_train}, pos_weight={pos_weight.item():.2f}\")\n",
    "    \n",
    "    # === 2. 构建模型 ===\n",
    "    model = BiLSTMModel(MODEL_CONFIG).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=TRAIN_CONFIG['lr'], weight_decay=TRAIN_CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)  # 监控val_loss\n",
    "    \n",
    "    # === 3. 训练循环 ===\n",
    "    best_loss = float('inf')  # 改为监控验证loss\n",
    "    patience_cnt = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_f1': []}\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    save_path = os.path.join(PATH_CONFIG['save_dir'], f\"bilstm_{timestamp}.pth\")\n",
    "    \n",
    "    logger.info(f\"Start training for {TRAIN_CONFIG['epochs']} epochs...\")\n",
    "    \n",
    "    for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # --- Validate ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true, y_probs = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.unsqueeze(1).to(device)\n",
    "                logits = model(x)\n",
    "                val_loss += criterion(logits, y).item()\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_probs.extend(probs)\n",
    "        \n",
    "        # --- Metrics ---\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        y_true = np.array(y_true).flatten()\n",
    "        y_probs = np.array(y_probs).flatten()\n",
    "        y_pred = (y_probs > 0.5).astype(int)\n",
    "        \n",
    "        val_auc = roc_auc_score(y_true, y_probs)\n",
    "        val_f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1:03d} | Loss: {avg_train_loss:.4f}/{avg_val_loss:.4f} | \"\n",
    "            f\"AUC: {val_auc:.4f} | F1: {val_f1:.4f} | Time: {time.time()-start_time:.1f}s\"\n",
    "        )\n",
    "        \n",
    "        scheduler.step(avg_val_loss)  # 监控验证loss\n",
    "        \n",
    "        # --- Early Stopping (基于验证loss，防止过拟合) ---\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_auc = val_auc  # 记录对应的AUC\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': MODEL_CONFIG,\n",
    "            }, save_path)\n",
    "            patience_cnt = 0\n",
    "            logger.info(f\"  -> Best model saved! (Loss: {best_loss:.4f}, AUC: {val_auc:.4f})\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= TRAIN_CONFIG['patience']:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # === 4. 最终评估 ===\n",
    "    evaluate_and_plot(save_path, val_loader, history)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model_path, dataloader, history):\n",
    "    \"\"\"加载最佳模型进行评估并绘制图表\"\"\"\n",
    "    logger.info(\"Running final evaluation...\")\n",
    "    \n",
    "    # 加载模型\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model = BiLSTMModel(checkpoint['config']).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 预测\n",
    "    y_true, y_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            y_true.extend(y.numpy())\n",
    "            y_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_probs = np.array(y_probs).flatten()\n",
    "    y_pred = (y_probs > 0.5).astype(int)\n",
    "    \n",
    "    # 打印指标\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EVALUATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"ROC AUC:   {roc_auc_score(y_true, y_probs):.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 绘图\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Loss曲线\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Curve')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC曲线\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc_val = roc_auc_score(y_true, y_probs)\n",
    "    axes[1].plot(fpr, tpr, label=f'AUC = {roc_auc_val:.3f}')\n",
    "    axes[1].plot([0, 1], [0, 1], 'r--')\n",
    "    axes[1].set_xlabel('FPR')\n",
    "    axes[1].set_ylabel('TPR')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR曲线\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_probs)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "    axes[2].plot(recall_vals, precision_vals, label=f'AUC = {pr_auc:.3f}')\n",
    "    axes[2].set_xlabel('Recall')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].set_title('PR Curve')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.pdf'))\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Figures saved to {PATH_CONFIG['fig_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 运行训练\n",
    "model_path = train()\n",
    "print(f\"\\nModel saved to: {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
