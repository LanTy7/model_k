{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM 多分类模型训练\n",
    "\n",
    "用于抗性基因(ARG)类别分类\n",
    "\n",
    "**关键改进**:\n",
    "1. Focal Loss 处理类别不平衡\n",
    "2. 分层采样确保每个batch包含各类样本\n",
    "3. 自动合并稀有类别\n",
    "4. 保存配置信息到模型文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 模型配置（预测时必须保持一致）==================\n",
    "MODEL_CONFIG = {\n",
    "    'embedding_size': 21,    # One-hot编码维度 (20氨基酸 + PAD)\n",
    "    'hidden_size': 128,      # LSTM隐藏层维度\n",
    "    'num_layers': 2,         # LSTM层数\n",
    "    'dropout': 0.4,          # 降低Dropout，防止欠拟合\n",
    "}\n",
    "\n",
    "# ================== 训练配置 ==================\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 256,       # 增大batch size，让loss更平滑\n",
    "    'lr': 0.002,             # 稍微提高初始学习率\n",
    "    'warmup_epochs': 5,      # 学习率预热轮数\n",
    "    'epochs': 150,\n",
    "    'patience': 25,          # 增加早停耐心值\n",
    "    'min_samples': 50,       # 少于此数的类别合并为Others\n",
    "    'focal_gamma': 0.5,      # 降低gamma，减少对困难样本的过度关注\n",
    "    'label_smoothing': 0.1,  # 标签平滑，让训练更稳定\n",
    "}\n",
    "\n",
    "# ================== 路径配置（请修改为实际路径）==================\n",
    "PATH_CONFIG = {\n",
    "    'fasta_file': \"/home/mayue/bilstm/data/changyn/unique_0.95.fasta\",  # ARG序列文件\n",
    "    'save_dir': \"./well-trained\",\n",
    "    'log_dir': \"./logs\",\n",
    "    'fig_dir': \"./figures\",\n",
    "}\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 创建目录\n",
    "for d in [PATH_CONFIG['save_dir'], PATH_CONFIG['log_dir'], PATH_CONFIG['fig_dir']]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 日志\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氨基酸编码字典\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_DICT = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "AA_DICT.update({\n",
    "    'B': [AA_DICT['D'], AA_DICT['N']],  # Asp or Asn\n",
    "    'Z': [AA_DICT['E'], AA_DICT['Q']],  # Glu or Gln\n",
    "    'J': [AA_DICT['I'], AA_DICT['L']],  # Ile or Leu\n",
    "    'X': 'ANY',\n",
    "    'PAD': 20\n",
    "})\n",
    "\n",
    "def one_hot_encode(sequence, max_length):\n",
    "    \"\"\"将氨基酸序列转换为one-hot编码\"\"\"\n",
    "    encoding = np.zeros((max_length, 21), dtype=np.float32)\n",
    "    for i in range(min(len(sequence), max_length)):\n",
    "        aa = sequence[i]\n",
    "        if aa in AA_DICT:\n",
    "            idx = AA_DICT[aa]\n",
    "            if isinstance(idx, list):  # 模糊氨基酸\n",
    "                for j in idx:\n",
    "                    encoding[i, j] = 0.5\n",
    "            elif idx == 'ANY':  # 未知氨基酸\n",
    "                encoding[i, :20] = 0.05\n",
    "            else:\n",
    "                encoding[i, idx] = 1.0\n",
    "        else:\n",
    "            encoding[i, :20] = 0.05  # 未知字符\n",
    "    # 填充位置标记为PAD\n",
    "    if len(sequence) < max_length:\n",
    "        encoding[len(sequence):, 20] = 1.0\n",
    "    return encoding\n",
    "\n",
    "\n",
    "class ARGDataset(Dataset):\n",
    "    \"\"\"ARG序列数据集\"\"\"\n",
    "    def __init__(self, sequences, labels, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = one_hot_encode(self.sequences[idx], self.max_length)\n",
    "        return torch.from_numpy(encoded), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss with Label Smoothing for imbalanced classification\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 类别权重\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, weight=self.alpha, \n",
    "            reduction='none', label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"BiLSTM + Global Pooling 多分类模型\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config['embedding_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=config['dropout'] if config['num_layers'] > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        # 双向LSTM输出 * 2 (max + avg pooling) = hidden_size * 4\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config['hidden_size'] * 4, config['hidden_size']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['hidden_size'], num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)  # (batch, seq_len, hidden*2)\n",
    "        \n",
    "        # Global Pooling\n",
    "        max_pool, _ = torch.max(output, dim=1)\n",
    "        avg_pool = torch.mean(output, dim=1)\n",
    "        features = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        \n",
    "        return self.classifier(self.dropout(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(fasta_file, min_samples):\n",
    "    \"\"\"加载FASTA文件并预处理数据\"\"\"\n",
    "    logger.info(f\"Loading data from {fasta_file}\")\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # 解析FASTA文件\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        try:\n",
    "            parts = record.description.split('|')\n",
    "            if len(parts) >= 4:\n",
    "                label = parts[3]\n",
    "                sequences.append(str(record.seq).upper())\n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Loaded {len(sequences)} sequences\")\n",
    "    \n",
    "    # 统计类别分布\n",
    "    label_counts = Counter(labels)\n",
    "    logger.info(f\"Original classes: {len(label_counts)}\")\n",
    "    \n",
    "    # 合并稀有类别\n",
    "    rare_classes = [c for c, count in label_counts.items() if count < min_samples]\n",
    "    if rare_classes:\n",
    "        logger.info(f\"Merging {len(rare_classes)} rare classes (< {min_samples} samples) into 'Others'\")\n",
    "        labels = [l if l not in rare_classes else 'Others' for l in labels]\n",
    "    \n",
    "    # 创建标签映射\n",
    "    unique_labels = sorted(set(labels))\n",
    "    if 'Others' in unique_labels:\n",
    "        unique_labels.remove('Others')\n",
    "        unique_labels.append('Others')  # Others放最后\n",
    "    \n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    \n",
    "    # 转换为数字标签\n",
    "    y = np.array([label_to_idx[l] for l in labels])\n",
    "    \n",
    "    # 计算max_length (95分位数)\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    max_length = int(np.percentile(lengths, 95))\n",
    "    logger.info(f\"Max length (95th percentile): {max_length}\")\n",
    "    \n",
    "    # 类别分布\n",
    "    class_counts = np.bincount(y)\n",
    "    logger.info(f\"Final classes: {len(unique_labels)}\")\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        logger.info(f\"  {label}: {class_counts[i]}\")\n",
    "    \n",
    "    return sequences, y, unique_labels, label_to_idx, idx_to_label, max_length, class_counts\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "sequences, y, class_names, label_to_idx, idx_to_label, max_length, class_counts = \\\n",
    "    load_and_preprocess_data(PATH_CONFIG['fasta_file'], TRAIN_CONFIG['min_samples'])\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nNum classes: {num_classes}\")\n",
    "print(f\"Max length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集/验证集 (分层采样)\n",
    "train_seqs, val_seqs, y_train, y_val = train_test_split(\n",
    "    sequences, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logger.info(f\"Train: {len(train_seqs)}, Val: {len(val_seqs)}\")\n",
    "\n",
    "# 创建Dataset\n",
    "train_dataset = ARGDataset(train_seqs, y_train, max_length)\n",
    "val_dataset = ARGDataset(val_seqs, y_val, max_length)\n",
    "\n",
    "# 创建DataLoader（移除分层采样，让Focal Loss单独处理类别不平衡）\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=TRAIN_CONFIG['batch_size'],\n",
    "    shuffle=True,  # 使用普通shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=TRAIN_CONFIG['batch_size'],\n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warmup_scheduler(optimizer, warmup_epochs, total_epochs, steps_per_epoch):\n",
    "    \"\"\"创建带有warmup的学习率调度器\"\"\"\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            # 线性warmup\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # 余弦退火\n",
    "            progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # === 1. 构建模型 ===\n",
    "    model = BiLSTMClassifier(MODEL_CONFIG, num_classes).to(device)\n",
    "    \n",
    "    # 计算类别权重（更温和的权重）\n",
    "    class_weights = torch.tensor(\n",
    "        np.sqrt(np.median(class_counts) / (class_counts + 1)),  # 使用平方根，减少极端权重\n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "    class_weights = torch.clamp(class_weights, 0.5, 3.0)  # 限制权重范围\n",
    "    logger.info(f\"Class weights range: [{class_weights.min():.2f}, {class_weights.max():.2f}]\")\n",
    "    \n",
    "    # Focal Loss with Label Smoothing\n",
    "    criterion = FocalLoss(\n",
    "        alpha=class_weights, \n",
    "        gamma=TRAIN_CONFIG['focal_gamma'],\n",
    "        label_smoothing=TRAIN_CONFIG['label_smoothing']\n",
    "    )\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=TRAIN_CONFIG['lr'], weight_decay=1e-4)\n",
    "    \n",
    "    # 使用Warmup + Cosine退火调度器\n",
    "    scheduler = get_warmup_scheduler(\n",
    "        optimizer, \n",
    "        TRAIN_CONFIG['warmup_epochs'], \n",
    "        TRAIN_CONFIG['epochs'],\n",
    "        len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # === 2. 训练循环 ===\n",
    "    best_acc = 0.0\n",
    "    patience_cnt = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    save_path = os.path.join(PATH_CONFIG['save_dir'], f\"bilstm_multi_{timestamp}.pth\")\n",
    "    \n",
    "    logger.info(f\"Start training for {TRAIN_CONFIG['epochs']} epochs...\")\n",
    "    \n",
    "    for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for x, y_batch in train_loader:\n",
    "            x, y_batch = x.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # 每个batch更新学习率\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
    "            train_total += y_batch.size(0)\n",
    "        \n",
    "        # --- Validate ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y_batch in val_loader:\n",
    "                x, y_batch = x.to(device), y_batch.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "        \n",
    "        # --- Metrics ---\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1:03d} | Loss: {avg_train_loss:.4f}/{avg_val_loss:.4f} | \"\n",
    "            f\"Acc: {train_acc:.4f}/{val_acc:.4f} | LR: {current_lr:.6f} | Time: {time.time()-start_time:.1f}s\"\n",
    "        )\n",
    "        \n",
    "        # --- Early Stopping ---\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_config': MODEL_CONFIG,\n",
    "                'class_names': class_names,\n",
    "                'label_to_idx': label_to_idx,\n",
    "                'max_length': max_length,\n",
    "            }, save_path)\n",
    "            patience_cnt = 0\n",
    "            logger.info(f\"  -> Best model saved! (Acc: {best_acc:.4f})\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= TRAIN_CONFIG['patience']:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # === 3. 评估 ===\n",
    "    evaluate_and_plot(save_path, val_loader, history)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估与可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model_path, dataloader, history):\n",
    "    \"\"\"加载最佳模型进行评估并绘制图表\"\"\"\n",
    "    logger.info(\"Running final evaluation...\")\n",
    "    \n",
    "    # 加载模型\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model = BiLSTMClassifier(checkpoint['model_config'], len(checkpoint['class_names'])).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 预测\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y_batch in dataloader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(y_true, y_pred, target_names=checkpoint['class_names']))\n",
    "    print(f\"Overall Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 绘图 (2x2布局)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Loss曲线\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curve')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy曲线\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学习率曲线\n",
    "    axes[1, 0].plot(history['lr'], color='green', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule (Warmup + Cosine)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=checkpoint['class_names'],\n",
    "                yticklabels=checkpoint['class_names'], ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('True')\n",
    "    axes[1, 1].set_title('Confusion Matrix')\n",
    "    plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(axes[1, 1].get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Figures saved to {PATH_CONFIG['fig_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 开始训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 运行训练\n",
    "model_path = train()\n",
    "print(f\"\\nModel saved to: {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gene_pred)",
   "language": "python",
   "name": "gene_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
